Things TODO:

>Agent:
- Integrate the working example with project
- flake8
- Define your custom callbacks
- Add all previous __main__ arguments such as --render, --stream
- What is the current batch size?

>Features:
- predict function to how the features work
- Batch, instance normalization?
- improve initizazion

Things done:
- Simple autoencoder
- Crop the game board to the relevant part
- Tensorforce for DQN
- Trained features and agent on server
- Player.play()
- Decaying exploration rate
- collect training samples during reinforcement learning

Notes:
- In breakout, rewards are given at each brocken brick 
- Normalization and initialization are important for deep nets!
- Be careful with the loss function for the autoencoder:
	the internal features should be meaningful (i.e. contain fluents).
	Also, do not train the autoencoder alone, but toghether with other losses.
	Use sparsity constraints (regularization loss?) (maybe not necessary with binary data)
	Prefer conv2d maxpooling and upsampling2d

