Things TODO:

>Agent:
+ Update keras-rl dependency
- Save previous runs
- Deterministic training with tf.keras
	from checkpoint. If they match, commit.
- Train with crop and train new agent
- Retrain

>Features:
- predict function to how the features work
- Batch, instance normalization?
- improve initizazion

Things done:
- Simple autoencoder
- Crop the game board to the relevant part
- Trained features and agent on server
- Player.play()
- Decaying exploration rate
- collect training samples during reinforcement learning

Notes:
- In breakout, rewards are given at each brocken brick 
- Normalization and initialization are important for deep nets!
- Be careful with the loss function for the autoencoder:
	the internal features should be meaningful (i.e. contain fluents).
	Also, do not train the autoencoder alone, but toghether with other losses.
	Use sparsity constraints (regularization loss?) (maybe not necessary with binary data)
	Prefer conv2d maxpooling and upsampling2d

